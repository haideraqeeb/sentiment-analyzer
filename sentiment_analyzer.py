# -*- coding: utf-8 -*-
"""sentiment analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124YZ3WO2wz1Mg780-Y6iqGq9qT4aspR1
"""

!pip install evaluate
!pip install datasets

import pandas as pd
import numpy as np
import transformers
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
import evaluate
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm.auto import tqdm
from huggingface_hub import notebook_login, HfApi

dataframe = pd.read_csv("/content/drive/MyDrive/twitter_sentiment_data.csv")
dataframe

dataframe = dataframe[dataframe["sentiment"] != 2]
dataframe = dataframe[dataframe["sentiment"] != 0]
dataframe["sentiment"] = dataframe["sentiment"].replace(-1, 0)
dataframe

dataframe = dataframe.drop(["tweetid"], axis=1)

train_df, test_df = train_test_split(dataframe, test_size=0.2, random_state=42, stratify=dataframe["sentiment"])

train_ds = Dataset.from_pandas(train_df, preserve_index=False)
test_ds = Dataset.from_pandas(test_df, preserve_index=False)

dataset = DatasetDict({
    "train": train_ds,
    "test": test_ds
})
dataset

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

def preprocessing_fn(data):
    preprocess_text = tokenizer(
        data["message"],
        truncation=True,
        padding='max_length',
        max_length=512,
        return_tensors="pt"
    )

    return preprocess_text

dataset = dataset.map(preprocessing_fn, batched=True)

dataset

dataset = dataset.remove_columns(["message"])
dataset = dataset.rename_column("sentiment", "labels")
dataset.set_format("torch")
dataset

train_ds = DataLoader(dataset["train"], shuffle=True, batch_size=16)
test_ds = DataLoader(dataset["test"], batch_size=16)

optimizer = AdamW(model.parameters(), lr=0.00001)

num_epochs = 3
num_training_steps = num_epochs * len(train_ds)

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
print(f"device used is {device}")

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_ds:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

model.eval()

metric = evaluate.load("accuracy")

for batch in test_ds:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
      outputs = model(**batch)

      logits = outputs.logits
      predictions = torch.argmax(logits, dim=-1)
      metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()

notebook_login()

save_path = "/content/drive/MyDrive/models/sentiment analyzer"

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

api = HfApi()

model_name = "environment_sentiment"
model_path = save_path

token = HfFolder.get_token()

api.create_repo(name=model_name, token=token)

api.upload_folder(
    folder_path=model_path,

)

import datasets
from torch.utils.data import DataLoader
from torch.optim import AdamW
from tqdm.auto import tqdm
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_scheduler
import evaluate
import torch
from datasets import load_dataset

dataset = load_dataset("climatebert/climate_sentiment")

dataset

train_ds = DataLoader(dataset["train"], shuffle=True, batch_size=16)
test_ds = DataLoader(dataset["train"], batch_size=16)

model = DistilBertForSequenceClassification.from_pretrained("distil-bert-uncased")
tokenizer = DistilBertTokenizer.from_pretrained("distil-bert-uncased")

